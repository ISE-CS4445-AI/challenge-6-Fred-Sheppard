{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISE-CS4445-AI/challenge-6-Fred-Sheppard/blob/main/challenge-6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSNohoRf9Wum"
      },
      "source": [
        "# Challenge #6: Reducing Overfitting with Regularization (FashionMNIST)\n",
        "\n",
        "**Instructions**:\n",
        "- This challenge builds on **Week 6**’s regularization lessons: **data augmentation**, **dropout**, and **weight decay**.\n",
        "- We demonstrate a baseline CNN on **FashionMNIST** that clearly overfits. Then we apply these techniques to see the improvement.\n",
        "\n",
        "**Scoring**:  9 points total. Fill in the `# TODO` placeholders and fill in the final reflection markdown cell. **There is no autograder** for this assignment. If your code works without errors and achieves the objectives of reducing overfitting by the end of this challenge, you will have done well. 👍\n",
        "\n",
        "We’ll use the **FashionMNIST** dataset, which consists of 60,000 training images and 10,000 test images of 28x28 grayscale images of 10 different fashion categories. By applying these techniques, we aim to reduce overfitting and improve robust performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UM1fVyy9Wun"
      },
      "source": [
        "## Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "LfptRzGQ9Wuo",
        "outputId": "7d79ba8f-9104-4b59-9564-afe45799f561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu124\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGhzcybk9Wuo"
      },
      "source": [
        "### Utility functions\n",
        "\n",
        "Defining the utility function to plot metrics below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "GNF9J7iy9Wuo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Description:\n",
        "Utility function to plot line plots in a single graph\n",
        "\n",
        "Params:\n",
        "dataDict: List - A list of tuples containing the data to be plotted and corresponding plot params as a dictionary (optional).\n",
        "           Expected key names in paramDict:\n",
        "           -> label: str - String label to be given to the plot.\n",
        "                           Only necessary if the 'legend' function param is set to True (False by default).\n",
        "           -> ha: str - Specifies the horizontal alignment ('left', 'right' or 'center') of text above each point in the plot.\n",
        "           -> fontsize: int - Sets the font size of text displayed above each point in the plot.\n",
        "           -> marker: str - Sets the style of marker to be displayed for each data point on the plot. Set to 'o' by default.\n",
        "           -> decimalPlaces: int - Sets the number of decimal places to display for each data point.\n",
        "           -> displayPercent: bool - Boolean to decide whether to display numbers in percentage format.\n",
        "           -> displayOffset: float - positive or negative float value that determines the display offset of text above data point.\n",
        "title: str - [optional] Title to be set for the graph.\n",
        "xlabel: str - [optional] Label for the x-axis to be set for the graph.\n",
        "ylabel: str - [optional] Label for the y-axis to be set for the graph.\n",
        "figSize: Tuple - [optional] Sets a custom figure size for the plot based on the width and height values passed as a tuple pair.\n",
        "legend: bool - [optional] Boolean to decide whether to show the legend or not. Set to False by default\n",
        "'''\n",
        "def plotMetrics(dataList, X, title='', xlabel='', ylabel='', figSize=None, legend=False):\n",
        "    if figSize:\n",
        "            plt.figure(figsize=(figSize))\n",
        "    for data in dataList:\n",
        "        y, paramDict = data\n",
        "        # Getting plot params\n",
        "        label = paramDict['label'] if 'label' in paramDict else ''\n",
        "        marker = paramDict['marker'] if 'marker' in paramDict else 'o'\n",
        "        ha = paramDict['ha'] if 'ha' in paramDict else 'center'\n",
        "        fontSize = paramDict['fontSize'] if 'fontSize' in paramDict else 8\n",
        "        decimalPlaces = paramDict['decimalPlaces'] if 'decimalPlaces' in paramDict else 2\n",
        "        displayPercent = paramDict['displayPercent'] if 'displayPercent' in paramDict else False\n",
        "        displayOffset = paramDict['displayOffset'] if 'displayOffset' in paramDict else 0.005\n",
        "\n",
        "        plt.plot(X, y, label=label, marker=marker)\n",
        "\n",
        "        # Getting the data values to show on the plotted points along the line\n",
        "        for i, v in enumerate(y):\n",
        "            percentMultiplier = 100 if displayPercent else 1\n",
        "            v_str = f'{v * percentMultiplier:.{decimalPlaces}f}{\"%\" if displayPercent else \"\"}'\n",
        "            plt.text(i + 1, v + displayOffset, v_str, ha=ha, fontsize=fontSize)\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    if legend:\n",
        "        plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJQ2coP99Wuo"
      },
      "source": [
        "## Task 1: getTransforms(augmentation=False) <font color='green'>(1 point)</font>\n",
        "\n",
        "**Goal**: Return a **train_transform** and a **test_transform**.  \n",
        "- If `augmentation=True`, apply random transformations. Otherwise just basic.  \n",
        "- *FashionMNIST* is 28×28 grayscale, so consider **random horizontal flip** or **random rotation**. Keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "F4db54aV9Wup"
      },
      "outputs": [],
      "source": [
        "def getTransforms(augmentation=False):\n",
        "    # mean,std for FashionMNIST (approx)\n",
        "    mean = (0.2860,)\n",
        "    std = (0.3530,)\n",
        "\n",
        "    # TODO: define the transformation for the test set (without any data augmentation)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    if augmentation:\n",
        "        # TODO: define the transformation for the train set (with data augmentation)\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    return test_transform, test_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK3NiUtTqhlG"
      },
      "source": [
        "## Task 2: create the dataloaders <font color='green'>(1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "l4_wowyh9Wup",
        "outputId": "1cfc5477-72ab-4ac3-fc12-f57b458d8190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 60000\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "def load_fashionMNIST_data(batch_size=64, augmentation=False):\n",
        "    \"\"\"\n",
        "    return train_loader, test_loader\n",
        "    \"\"\"\n",
        "\n",
        "    train_transform, test_transform = getTransforms(augmentation)\n",
        "\n",
        "    # TODO: download the FashionMNIST dataset\n",
        "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', download=True, transform=train_transform, train=True)\n",
        "    test_dataset  = torchvision.datasets.FashionMNIST(root='./data', download=True, transform=test_transform, train=False)\n",
        "\n",
        "    # TODO: create the data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = load_fashionMNIST_data(batch_size=64)\n",
        "print(\"Train set size:\", len(train_loader.dataset))\n",
        "print(\"Test set size:\", len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoYFlQ5S9Wup"
      },
      "source": [
        "## Task 3: Define a CNN with Dropout  <font color='green'>(2 points)</font>\n",
        "\n",
        "**Task**:  \n",
        "- Build a small **CNN** with a few convolution layers, each followed by ReLU & maxpool.  \n",
        "- Insert **dropout** layers (e.g., `nn.Dropout(0.3)`) to help reduce overfitting.  \n",
        "- The final linear outputs 10 classes.  \n",
        "- We'll call it `NetFashion`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ZEUmlcAe9Wup"
      },
      "outputs": [],
      "source": [
        "class NetFashion(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.3, useRegularization=False):\n",
        "        super().__init__()\n",
        "        self.useRegularization = useRegularization\n",
        "        # TODO: Build a sample CNN architecture (refer architecture from week 6 exercise notebook if needed)\n",
        "        # Note that the FashionMNIST dataset has 10 classes, input size of 28x28 and 1 channel (greyscale)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.pool  = nn.MaxPool2d(2,2)\n",
        "\n",
        "        if useRegularization:\n",
        "            self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape x: (batch,3,32,32)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # (batch,64,16,16)\n",
        "\n",
        "        # Use regularisation layers as you see fit\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        out = self.fc2(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1QzXFmP9Wup"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Below is a standard training loop with helper train and test methods defined which should look familiar by now. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "_usTUEoh9Wuq"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimiser):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # This is the backprop set up. Explain what each of the steps do\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    loss, accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    loss /= num_batches\n",
        "    accuracy /= size\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "def train_loop(train_dataloader, test_dataloader, model, loss_fn, optimiser, epochs):\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    test_loss = []\n",
        "    test_accuracy = []\n",
        "\n",
        "    # Iterate over each epoch\n",
        "    for t in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        print(f\"Epoch {t+1}:\\n\")\n",
        "        train(train_dataloader, model, loss_fn, optimiser)\n",
        "\n",
        "        # Get the overall loss and accuracy for both train and test datasets\n",
        "        tr_loss, tr_acc = test(train_dataloader, model, loss_fn)\n",
        "        ts_loss, ts_acc = test(test_dataloader, model, loss_fn)\n",
        "\n",
        "        print(f\"Train Error: \\n Accuracy: {(100*tr_acc):>0.1f}%, Avg loss: {tr_loss:>8f} \\n\")\n",
        "        print(f\"Test Error: \\n Accuracy: {(100*ts_acc):>0.1f}%, Avg loss: {ts_loss:>8f} \\n\")\n",
        "\n",
        "        # Store and return the losses and accuracies. We can graph these later\n",
        "        train_loss = train_loss + [tr_loss]\n",
        "        train_accuracy = train_accuracy + [tr_acc]\n",
        "        test_loss = test_loss + [ts_loss]\n",
        "        test_accuracy = test_accuracy + [ts_acc]\n",
        "\n",
        "    print(\"Done training!\")\n",
        "    return train_loss, train_accuracy, test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skld-1ZY9Wuq"
      },
      "source": [
        "## Task 4: Evaluate & Check Test Accuracy\n",
        "\n",
        "**Task**:  \n",
        "1. Measure train and test metrics for non-regularised model with non-augmented data.  \n",
        "2. Do the same train and test loop for your regularised model on augmented data with weight decay in the optimiser and observe the difference.\n",
        "   1. However, apply all the different techniques individually.\n",
        "   2. Run the code cells to plot metrics after each experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYtL_HwI9Wuq"
      },
      "source": [
        "### Task 4.1: Non-regularised model with non-augmented data  <font color='green'>(1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "tFlj0ub59Wuq"
      },
      "outputs": [],
      "source": [
        "model = NetFashion().to(device) # Getting the Fashion MNIST model\n",
        "\n",
        "# TODO: Define the loss function and the optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjcJ-oFe9Wuq",
        "collapsed": true,
        "outputId": "d824316b-36de-4f5e-95a9-eb936b01f0ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\n",
            "loss: 2.290135  [   64/60000]\n",
            "loss: 0.609140  [ 6464/60000]\n",
            "loss: 0.460987  [12864/60000]\n",
            "loss: 0.423722  [19264/60000]\n",
            "loss: 0.378305  [25664/60000]\n",
            "loss: 0.225114  [32064/60000]\n",
            "loss: 0.426869  [38464/60000]\n",
            "loss: 0.172476  [44864/60000]\n",
            "loss: 0.271275  [51264/60000]\n",
            "loss: 0.386114  [57664/60000]\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG0oDYXB9Wuq"
      },
      "source": [
        "Plotting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy-10ibk9Wuq"
      },
      "outputs": [],
      "source": [
        "epochRange = range(1, epochs+1)\n",
        "# Defining data and plot params\n",
        "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': 0.01}),\n",
        "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.012})]\n",
        "plotTitle = 'Train and Test Loss for regularised model'\n",
        "\n",
        "# Calling my custom util function to plot loss data\n",
        "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkeVAhHC9Wuq"
      },
      "outputs": [],
      "source": [
        "# Defining data and plot params\n",
        "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': 0.01,\n",
        "                                      'decimalPlaces': 2, 'displayPercent': True}),\n",
        "                (test_accuracy, {'label': 'Test accuracy', 'displayOffset': -0.01,\n",
        "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
        "plotTitle = 'Train and Test Accuracy for non-regularised model'\n",
        "\n",
        "# Calling my custom util function to plot accuracy data\n",
        "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgtMLLyO9Wuq"
      },
      "source": [
        "### Task 4.2: With regularisation and data augmentation applied <font color='green'>(3 points)</font>\n",
        "Train and test the same model with different regularisation techniques applied where you apply them one at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilZR5KEl9Wuq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Apply only data augmentation\n",
        "train_loader, test_loader = load_fashionMNIST_data(batch_size=64, augmentation=True)\n",
        "\n",
        "model = NetFashion().to(device) # Getting the Fashion MNIST model\n",
        "\n",
        "epochs = 10 # Increasing the number of epochs by 5 since the learning will be more gradual.\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzHYaQOo9Wuq"
      },
      "source": [
        "Plotting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-DC2jQJ9Wuq"
      },
      "outputs": [],
      "source": [
        "epochRange = range(1, epochs+1)\n",
        "# Defining data and plot params\n",
        "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': 0.03, 'ha': 'left'}),\n",
        "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.045, 'ha': 'right'})]\n",
        "plotTitle = 'Train and Test Loss for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot loss data\n",
        "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkcbUXC29Wuq"
      },
      "outputs": [],
      "source": [
        "# Defining data and plot params\n",
        "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': -0.012,\n",
        "                                      'decimalPlaces': 2, 'displayPercent': True, 'ha': 'left'}),\n",
        "                (test_accuracy, {'label': 'Test accuracy', 'displayOffset': 0.005,\n",
        "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
        "plotTitle = 'Train and Test Accuracy for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot accuracy data\n",
        "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HsCxDci8qhlI"
      },
      "outputs": [],
      "source": [
        "# Apply only regularisation on the cnn model\n",
        "train_loader, test_loader = load_fashionMNIST_data(batch_size=64)\n",
        "\n",
        "model = NetFashion(useRegularization=True).to(device) # Getting the Fashion MNIST model\n",
        "epochs = 10 # Increasing the number of epochs by 5 since the learning will be more gradual.\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDdo-0MiqhlI"
      },
      "source": [
        "Plotting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3fvLym_qhlI"
      },
      "outputs": [],
      "source": [
        "epochRange = range(1, epochs+1)\n",
        "# Defining data and plot params\n",
        "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': 0.03, 'ha': 'left'}),\n",
        "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.045, 'ha': 'right'})]\n",
        "plotTitle = 'Train and Test Loss for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot loss data\n",
        "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hHt5mboqhlI"
      },
      "outputs": [],
      "source": [
        "# Defining data and plot params\n",
        "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': -0.012,\n",
        "                                      'decimalPlaces': 2, 'displayPercent': True, 'ha': 'left'}),\n",
        "                (test_accuracy, {'label': 'Test accuracy', 'displayOffset': 0.005,\n",
        "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
        "plotTitle = 'Train and Test Accuracy for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot accuracy data\n",
        "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ir75Ou8LqhlI"
      },
      "outputs": [],
      "source": [
        "# Apply only weight decay technique\n",
        "train_loader, test_loader = load_fashionMNIST_data(batch_size=64)\n",
        "\n",
        "model = NetFashion(useRegularization=True).to(device) # Getting the Fashion MNIST model\n",
        "\n",
        "epochs = 10 # Increasing the number of epochs by 5 since the learning will be more gradual.\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPYg4rNBqhlI"
      },
      "source": [
        "Plotting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtR6kHwqhlI"
      },
      "outputs": [],
      "source": [
        "epochRange = range(1, epochs+1)\n",
        "# Defining data and plot params\n",
        "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': 0.03, 'ha': 'left'}),\n",
        "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.045, 'ha': 'right'})]\n",
        "plotTitle = 'Train and Test Loss for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot loss data\n",
        "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M1wXgFHqhlJ"
      },
      "outputs": [],
      "source": [
        "# Defining data and plot params\n",
        "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': -0.012,\n",
        "                                      'decimalPlaces': 2, 'displayPercent': True, 'ha': 'left'}),\n",
        "                (test_accuracy, {'label': 'Test accuracy', 'displayOffset': 0.005,\n",
        "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
        "plotTitle = 'Train and Test Accuracy for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot accuracy data\n",
        "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYk7BVolqhlJ"
      },
      "source": [
        "Apply all the regularisation techniques together below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bgkctt9rqhlJ"
      },
      "outputs": [],
      "source": [
        "# Apply all regularisation techniques\n",
        "train_loader, test_loader = load_fashionMNIST_data(batch_size=64)\n",
        "\n",
        "model = NetFashion(useRegularization=True, dropout_prob=0.3).to(device) # Getting the Fashion MNIST model\n",
        "epochs = 10 # Increasing the number of epochs by 5 since the learning will be more gradual.\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpVsyG33qhlJ"
      },
      "source": [
        "Plotting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bZStRhqhlJ"
      },
      "outputs": [],
      "source": [
        "epochRange = range(1, epochs+1)\n",
        "# Defining data and plot params\n",
        "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': 0.03, 'ha': 'left'}),\n",
        "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.045, 'ha': 'right'})]\n",
        "plotTitle = 'Train and Test Loss for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot loss data\n",
        "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_rJ24jIqhlJ"
      },
      "outputs": [],
      "source": [
        "# Defining data and plot params\n",
        "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': -0.012,\n",
        "                                      'decimalPlaces': 2, 'displayPercent': True, 'ha': 'left'}),\n",
        "                (test_accuracy, {'label': 'Test accuracy', 'displayOffset': 0.005,\n",
        "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
        "plotTitle = 'Train and Test Accuracy for Regularised model'\n",
        "\n",
        "# Calling my custom util function to plot accuracy data\n",
        "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True, figSize=(10, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcQCvoiH9Wuq"
      },
      "source": [
        "## Reflection <font color='green'>(1 point)</font>\n",
        "Write your thoughts and observations about overfitting and the techniques used above to reduce them in brief.\n",
        "\n",
        "This excersize showed how overfitting can be spotted in the wild. In the initial example, the train and test curves are spaced apart. There is a high training accuracy (95%) but the test accuracy is less (92%). Each form of regularisation that is used decreases the train-test gap until the two lines are almost equal. (92%, 91%). Weight decay seemed particulary effective, while the CNN regularisation has less of an impact. Clearly, regularisation is something that needs to be considered when training a CNN.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}